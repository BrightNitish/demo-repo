{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xZAjbZV6Huhq"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/data.zip -d /content/custom_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "metadata": {
        "id": "qJt3juXAL0TA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_to_mask(txt_file, image_shape, multi_class=True):\n",
        "    \"\"\"\n",
        "    Convert YOLO polygon format to segmentation mask\n",
        "\n",
        "    Args:\n",
        "        txt_file: Path to YOLO annotation file\n",
        "        image_shape: (height, width) of the image\n",
        "        multi_class: If True, different classes get different pixel values\n",
        "\n",
        "    Returns:\n",
        "        Binary or multi-class mask\n",
        "    \"\"\"\n",
        "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
        "\n",
        "    if not os.path.exists(txt_file):\n",
        "        return mask\n",
        "\n",
        "    with open(txt_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 6:  # Skip invalid lines\n",
        "                continue\n",
        "\n",
        "            class_id = int(parts[0])\n",
        "            coords = [float(x) for x in parts[1:]]\n",
        "\n",
        "            # Convert normalized coordinates to pixel coordinates\n",
        "            points = []\n",
        "            for i in range(0, len(coords), 2):\n",
        "                x = int(coords[i] * image_shape[1])\n",
        "                y = int(coords[i+1] * image_shape[0])\n",
        "                points.append([x, y])\n",
        "\n",
        "            if len(points) > 2:  # Need at least 3 points for a polygon\n",
        "                # Fill polygon - use class_id+1 for multi-class, 255 for binary\n",
        "                fill_value = (class_id + 1) if multi_class else 255\n",
        "                cv2.fillPoly(mask, [np.array(points)], fill_value)\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "iCvkofsFL9Xt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(source_dir, output_dir, multi_class=True):\n",
        "    \"\"\"\n",
        "    Process entire dataset: convert YOLO annotations to masks\n",
        "\n",
        "    Expected structure:\n",
        "    source_dir/\n",
        "    ├── images/\n",
        "    │   ├── image1.jpg\n",
        "    │   └── image2.jpg\n",
        "    └── labels/\n",
        "        ├── image1.txt\n",
        "        └── image2.txt\n",
        "    \"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, 'masks'), exist_ok=True)\n",
        "\n",
        "    image_dir = os.path.join(source_dir, 'images')\n",
        "    label_dir = os.path.join(source_dir, 'labels')\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    print(f\"Processing {len(image_files)} images...\")\n",
        "\n",
        "    for img_file in tqdm(image_files):\n",
        "        # Load image\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"Warning: Could not load {img_file}\")\n",
        "            continue\n",
        "\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # Find corresponding annotation file\n",
        "        base_name = os.path.splitext(img_file)[0]\n",
        "        txt_file = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "\n",
        "        # Convert to mask\n",
        "        mask = yolo_to_mask(txt_file, (height, width), multi_class)\n",
        "\n",
        "        # Save image and mask\n",
        "        output_img_path = os.path.join(output_dir, 'images', img_file)\n",
        "        output_mask_path = os.path.join(output_dir, 'masks', f\"{base_name}.png\")\n",
        "\n",
        "        # Copy image\n",
        "        shutil.copy2(img_path, output_img_path)\n",
        "\n",
        "        # Save mask\n",
        "        cv2.imwrite(output_mask_path, mask)\n",
        "\n",
        "    print(f\"Dataset processed! Output saved to: {output_dir}\")\n"
      ],
      "metadata": {
        "id": "s3DyG9iXJKJk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(processed_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Split processed dataset into train/val/test sets\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all image files\n",
        "    image_dir = os.path.join(processed_dir, 'images')\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Split dataset\n",
        "    train_files, temp_files = train_test_split(image_files, test_size=(1-train_ratio), random_state=42)\n",
        "    val_files, test_files = train_test_split(temp_files, test_size=(test_ratio/(val_ratio+test_ratio)), random_state=42)\n",
        "\n",
        "    print(f\"Dataset split:\")\n",
        "    print(f\"Train: {len(train_files)} images\")\n",
        "    print(f\"Val: {len(val_files)} images\")\n",
        "    print(f\"Test: {len(test_files)} images\")\n",
        "\n",
        "    # Create split directories\n",
        "    for split, files in [('train', train_files), ('val', val_files), ('test', test_files)]:\n",
        "        # Create directories\n",
        "        split_img_dir = os.path.join(processed_dir, split, 'images')\n",
        "        split_mask_dir = os.path.join(processed_dir, split, 'masks')\n",
        "        os.makedirs(split_img_dir, exist_ok=True)\n",
        "        os.makedirs(split_mask_dir, exist_ok=True)\n",
        "\n",
        "        # Copy files\n",
        "        for file in files:\n",
        "            base_name = os.path.splitext(file)[0]\n",
        "\n",
        "            # Copy image\n",
        "            src_img = os.path.join(processed_dir, 'images', file)\n",
        "            dst_img = os.path.join(split_img_dir, file)\n",
        "            shutil.copy2(src_img, dst_img)\n",
        "\n",
        "            # Copy mask\n",
        "            src_mask = os.path.join(processed_dir, 'masks', f\"{base_name}.png\")\n",
        "            dst_mask = os.path.join(split_mask_dir, f\"{base_name}.png\")\n",
        "            if os.path.exists(src_mask):\n",
        "                shutil.copy2(src_mask, dst_mask)\n"
      ],
      "metadata": {
        "id": "QVI4wijtPPSJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToothDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, multi_class=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.multi_class = multi_class\n",
        "\n",
        "        # Get all image files\n",
        "        self.images = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        # Filter images that have corresponding masks\n",
        "        valid_images = []\n",
        "        for img_file in self.images:\n",
        "            base_name = os.path.splitext(img_file)[0]\n",
        "            mask_file = os.path.join(mask_dir, f\"{base_name}.png\")\n",
        "            if os.path.exists(mask_file):\n",
        "                valid_images.append(img_file)\n",
        "\n",
        "        self.images = valid_images\n",
        "        print(f\"Found {len(self.images)} valid image-mask pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Corresponding mask\n",
        "        base_name = os.path.splitext(img_name)[0]\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{base_name}.png\")\n",
        "\n",
        "        # Load image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        image = np.array(image)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # Normalize mask\n",
        "        if self.multi_class:\n",
        "            # Keep class labels as integers\n",
        "            mask = mask.long()\n",
        "        else:\n",
        "            # Binary segmentation: normalize to 0-1\n",
        "            mask = mask.float() / 255.0\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "10ds2GQIPUPk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super(Up, self).__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Handle size mismatch\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "1DxofemlPcnS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=1, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "NvmUyznvPd3Y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = torch.sigmoid(pred)\n",
        "\n",
        "        # Flatten\n",
        "        pred = pred.view(-1)\n",
        "        target = target.view(-1)\n",
        "\n",
        "        intersection = (pred * target).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        ce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, dice_weight=0.5, focal_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.focal_loss = FocalLoss()\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        dice = self.dice_loss(pred, target)\n",
        "        focal = self.focal_loss(pred, target)\n",
        "        return self.dice_weight * dice + self.focal_weight * focal\n",
        "\n"
      ],
      "metadata": {
        "id": "82vA_4ByPhX_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms(image_size=512):\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(image_size, image_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "        A.GaussianBlur(blur_limit=3, p=0.1),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "        A.OneOf([\n",
        "            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
        "            A.GridDistortion(p=0.5),\n",
        "            A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.5),\n",
        "        ], p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(image_size, image_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n"
      ],
      "metadata": {
        "id": "QjDx8RR3PpJJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001, save_path='best_model.pth'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = CombinedLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "        for images, masks in train_bar:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
        "            for images, masks in val_bar:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "            }, save_path)\n",
        "            print(f'New best model saved! Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses\n"
      ],
      "metadata": {
        "id": "gCtKtALVPvI3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(pred, target, threshold=0.5):\n",
        "    \"\"\"Calculate Intersection over Union (IoU)\"\"\"\n",
        "    pred = (torch.sigmoid(pred) > threshold).float()\n",
        "    target = target.float()\n",
        "\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - intersection\n",
        "\n",
        "    if union == 0:\n",
        "        return 1.0\n",
        "    return (intersection / union).item()\n",
        "\n",
        "def calculate_dice(pred, target, threshold=0.5):\n",
        "    \"\"\"Calculate Dice coefficient\"\"\"\n",
        "    pred = (torch.sigmoid(pred) > threshold).float()\n",
        "    target = target.float()\n",
        "\n",
        "    intersection = (pred * target).sum()\n",
        "    dice = (2. * intersection) / (pred.sum() + target.sum())\n",
        "\n",
        "    return dice.item()\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    ious = []\n",
        "    dices = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(test_loader, desc='Evaluating'):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i in range(outputs.size(0)):\n",
        "                iou = calculate_iou(outputs[i], masks[i])\n",
        "                dice = calculate_dice(outputs[i], masks[i])\n",
        "                ious.append(iou)\n",
        "                dices.append(dice)\n",
        "\n",
        "    return np.mean(ious), np.mean(dices)\n"
      ],
      "metadata": {
        "id": "u6GfvKSGP2mE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory-Optimized Tooth Segmentation Pipeline\n",
        "# Designed to run on limited RAM systems\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ================================\n",
        "# MEMORY-OPTIMIZED DATASET PROCESSING\n",
        "# ================================\n",
        "\n",
        "def yolo_to_mask(txt_file, image_shape, multi_class=False):\n",
        "    \"\"\"\n",
        "    Convert YOLO polygon format to segmentation mask\n",
        "    Memory optimized version\n",
        "    \"\"\"\n",
        "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
        "\n",
        "    if not os.path.exists(txt_file):\n",
        "        return mask\n",
        "\n",
        "    try:\n",
        "        with open(txt_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) < 6:\n",
        "                    continue\n",
        "\n",
        "                class_id = int(parts[0])\n",
        "                coords = [float(x) for x in parts[1:]]\n",
        "\n",
        "                # Convert normalized coordinates to pixel coordinates\n",
        "                points = []\n",
        "                for i in range(0, len(coords), 2):\n",
        "                    x = int(coords[i] * image_shape[1])\n",
        "                    y = int(coords[i+1] * image_shape[0])\n",
        "                    points.append([x, y])\n",
        "\n",
        "                if len(points) > 2:\n",
        "                    fill_value = (class_id + 1) if multi_class else 255\n",
        "                    cv2.fillPoly(mask, [np.array(points)], fill_value)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {txt_file}: {e}\")\n",
        "\n",
        "    return mask\n",
        "\n",
        "def process_dataset_memory_efficient(source_dir, output_dir, multi_class=False, max_size=512):\n",
        "    \"\"\"\n",
        "    Process dataset with memory optimization\n",
        "    \"\"\"\n",
        "    # Create output directories\n",
        "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, 'masks'), exist_ok=True)\n",
        "\n",
        "    image_dir = os.path.join(source_dir, 'images')\n",
        "    label_dir = os.path.join(source_dir, 'labels')\n",
        "\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"Error: {image_dir} does not exist\")\n",
        "        return\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    print(f\"Processing {len(image_files)} images...\")\n",
        "    processed_count = 0\n",
        "\n",
        "    for i, img_file in enumerate(tqdm(image_files, desc=\"Converting YOLO to masks\")):\n",
        "        try:\n",
        "            # Load image\n",
        "            img_path = os.path.join(image_dir, img_file)\n",
        "            image = cv2.imread(img_path)\n",
        "\n",
        "            if image is None:\n",
        "                continue\n",
        "\n",
        "            height, width = image.shape[:2]\n",
        "\n",
        "            # Resize if too large to save memory\n",
        "            if max(height, width) > max_size:\n",
        "                scale = max_size / max(height, width)\n",
        "                new_width = int(width * scale)\n",
        "                new_height = int(height * scale)\n",
        "                image = cv2.resize(image, (new_width, new_height))\n",
        "                height, width = new_height, new_width\n",
        "\n",
        "            # Find corresponding annotation file\n",
        "            base_name = os.path.splitext(img_file)[0]\n",
        "            # Handle different annotation file naming conventions\n",
        "            possible_txt_files = [\n",
        "                f\"{base_name}.txt\",\n",
        "                f\"{base_name}_png.rf.{base_name.split('.')[-1] if '.' in base_name else 'txt'}.txt\"\n",
        "            ]\n",
        "\n",
        "            txt_file = None\n",
        "            for txt_name in possible_txt_files:\n",
        "                txt_path = os.path.join(label_dir, txt_name)\n",
        "                if os.path.exists(txt_path):\n",
        "                    txt_file = txt_path\n",
        "                    break\n",
        "\n",
        "            if txt_file is None:\n",
        "                # Try finding any txt file with similar name\n",
        "                for f in os.listdir(label_dir):\n",
        "                    if base_name in f and f.endswith('.txt'):\n",
        "                        txt_file = os.path.join(label_dir, f)\n",
        "                        break\n",
        "\n",
        "            # Convert to mask\n",
        "            mask = yolo_to_mask(txt_file, (height, width), multi_class)\n",
        "\n",
        "            # Save image and mask\n",
        "            output_img_path = os.path.join(output_dir, 'images', img_file)\n",
        "            output_mask_path = os.path.join(output_dir, 'masks', f\"{base_name}.png\")\n",
        "\n",
        "            # Save resized image\n",
        "            cv2.imwrite(output_img_path, image)\n",
        "            cv2.imwrite(output_mask_path, mask)\n",
        "\n",
        "            processed_count += 1\n",
        "\n",
        "            # Clear memory every 10 images\n",
        "            if i % 10 == 0:\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Successfully processed {processed_count} images\")\n",
        "\n",
        "# ================================\n",
        "# MEMORY-EFFICIENT DATASET CLASS\n",
        "# ================================\n",
        "\n",
        "class ToothDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, multi_class=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.multi_class = multi_class\n",
        "\n",
        "        # Get all image files\n",
        "        self.images = []\n",
        "        if os.path.exists(image_dir):\n",
        "            all_images = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "            # Filter images that have corresponding masks\n",
        "            for img_file in all_images:\n",
        "                base_name = os.path.splitext(img_file)[0]\n",
        "                mask_file = os.path.join(mask_dir, f\"{base_name}.png\")\n",
        "                if os.path.exists(mask_file):\n",
        "                    self.images.append(img_file)\n",
        "\n",
        "        print(f\"Found {len(self.images)} valid image-mask pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Corresponding mask\n",
        "        base_name = os.path.splitext(img_name)[0]\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{base_name}.png\")\n",
        "\n",
        "        # Load image and mask\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # Normalize mask\n",
        "        if self.multi_class:\n",
        "            mask = mask.long()\n",
        "        else:\n",
        "            mask = mask.float() / 255.0\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# ================================\n",
        "# LIGHTWEIGHT U-NET ARCHITECTURE\n",
        "# ================================\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class LightweightUNet(nn.Module):\n",
        "    \"\"\"Lightweight U-Net for memory-constrained environments\"\"\"\n",
        "    def __init__(self, n_channels=3, n_classes=1):\n",
        "        super(LightweightUNet, self).__init__()\n",
        "\n",
        "        # Reduced channel sizes for memory efficiency\n",
        "        self.inc = DoubleConv(n_channels, 32)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(32, 64))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 256))\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.up_conv1 = DoubleConv(256 + 128, 128)  # 256 (from x4) + 128 (from up1) = 384\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.up_conv2 = DoubleConv(128 + 64, 64)    # 128 (from x3) + 64 (from up2) = 192\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.up_conv3 = DoubleConv(64 + 32, 32)     # 64 (from x2) + 32 (from up3) = 96\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2)\n",
        "        self.up_conv4 = DoubleConv(32 + 32, 32)\n",
        "\n",
        "        self.outc = nn.Conv2d(32, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.bottleneck(x4)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up1(x5)\n",
        "        x = torch.cat([x, x4], dim=1)\n",
        "        x = self.up_conv1(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.up_conv2(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.up_conv3(x)\n",
        "\n",
        "        x = self.up4(x)\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.up_conv4(x)\n",
        "\n",
        "        return self.outc(x)\n",
        "\n",
        "# ================================\n",
        "# MEMORY-EFFICIENT TRAINING\n",
        "# ================================\n",
        "\n",
        "def train_model_memory_efficient(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Use mixed precision training if available\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Simple BCE loss for memory efficiency\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            if len(masks.shape) == 3:  # [batch, height, width]\n",
        "             masks = masks.unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, masks)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Clear cache\n",
        "            del outputs, loss\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "                if len(masks.shape) == 3:  # [batch, height, width]\n",
        "                   masks = masks.unsqueeze(1)\n",
        "\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        outputs = model(images)\n",
        "                        loss = criterion(outputs, masks)\n",
        "                else:\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, masks)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Clear cache\n",
        "                del outputs, loss\n",
        "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_tooth_model.pth')\n",
        "            print(f'New best model saved!')\n",
        "\n",
        "        # Force garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "# ================================\n",
        "# SIMPLE TRANSFORMS\n",
        "# ================================\n",
        "\n",
        "def get_simple_transforms(image_size=256):\n",
        "    \"\"\"Simple transforms to reduce memory usage\"\"\"\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(image_size, image_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(image_size, image_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# ================================\n",
        "# MAIN EXECUTION (MEMORY OPTIMIZED)\n",
        "# ================================\n",
        "\n",
        "def main():\n",
        "    # Conservative configuration for low memory\n",
        "    CONFIG = {\n",
        "        'source_dir': '/content/custom_data/train',\n",
        "        'processed_dir': '/content/processed_data',\n",
        "        'image_size': 256,  # Reduced from 512\n",
        "        'batch_size': 2,    # Reduced from 8\n",
        "        'num_epochs': 10,   # Reduced from 100\n",
        "        'learning_rate': 0.001,\n",
        "        'multi_class': False,\n",
        "        'max_size': 512,    # Max image size during processing\n",
        "    }\n",
        "\n",
        "    print(\"=== MEMORY-OPTIMIZED TOOTH SEGMENTATION ===\")\n",
        "\n",
        "    # Step 1: Process dataset\n",
        "    print(\"Step 1: Converting YOLO annotations to masks...\")\n",
        "    process_dataset_memory_efficient(\n",
        "        CONFIG['source_dir'],\n",
        "        CONFIG['processed_dir'],\n",
        "        CONFIG['multi_class'],\n",
        "        CONFIG['max_size']\n",
        "    )\n",
        "\n",
        "    # Step 2: Split dataset\n",
        "    print(\"\\nStep 2: Splitting dataset...\")\n",
        "    image_dir = os.path.join(CONFIG['processed_dir'], 'images')\n",
        "    if os.path.exists(image_dir):\n",
        "        image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        train_files, temp_files = train_test_split(image_files, test_size=0.3, random_state=42)\n",
        "        val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
        "\n",
        "        # Create split directories\n",
        "        for split, files in [('train', train_files), ('val', val_files), ('test', test_files)]:\n",
        "            split_img_dir = os.path.join(CONFIG['processed_dir'], split, 'images')\n",
        "            split_mask_dir = os.path.join(CONFIG['processed_dir'], split, 'masks')\n",
        "            os.makedirs(split_img_dir, exist_ok=True)\n",
        "            os.makedirs(split_mask_dir, exist_ok=True)\n",
        "\n",
        "            for file in files:\n",
        "                base_name = os.path.splitext(file)[0]\n",
        "\n",
        "                # Copy image\n",
        "                src_img = os.path.join(CONFIG['processed_dir'], 'images', file)\n",
        "                dst_img = os.path.join(split_img_dir, file)\n",
        "                if os.path.exists(src_img):\n",
        "                    shutil.copy2(src_img, dst_img)\n",
        "\n",
        "                # Copy mask\n",
        "                src_mask = os.path.join(CONFIG['processed_dir'], 'masks', f\"{base_name}.png\")\n",
        "                dst_mask = os.path.join(split_mask_dir, f\"{base_name}.png\")\n",
        "                if os.path.exists(src_mask):\n",
        "                    shutil.copy2(src_mask, dst_mask)\n",
        "\n",
        "    # Step 3: Create datasets\n",
        "    print(\"\\nStep 3: Creating datasets...\")\n",
        "    train_transform, val_transform = get_simple_transforms(CONFIG['image_size'])\n",
        "\n",
        "    train_dataset = ToothDataset(\n",
        "        os.path.join(CONFIG['processed_dir'], 'train', 'images'),\n",
        "        os.path.join(CONFIG['processed_dir'], 'train', 'masks'),\n",
        "        train_transform\n",
        "    )\n",
        "\n",
        "    val_dataset = ToothDataset(\n",
        "        os.path.join(CONFIG['processed_dir'], 'val', 'images'),\n",
        "        os.path.join(CONFIG['processed_dir'], 'val', 'masks'),\n",
        "        val_transform\n",
        "    )\n",
        "\n",
        "    # Use num_workers=0 to avoid multiprocessing issues\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
        "\n",
        "    # Step 4: Train model\n",
        "    print(\"\\nStep 4: Training model...\")\n",
        "    model = LightweightUNet(n_channels=3, n_classes=1)\n",
        "\n",
        "    train_model_memory_efficient(\n",
        "        model, train_loader, val_loader,\n",
        "        CONFIG['num_epochs'], CONFIG['learning_rate']\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== TRAINING COMPLETED ===\")\n",
        "    print(\"Model saved as 'best_tooth_model.pth'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYtcje8-P-I5",
        "outputId": "8e34adef-bba4-498c-b991-2423a8ac8404"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MEMORY-OPTIMIZED TOOTH SEGMENTATION ===\n",
            "Step 1: Converting YOLO annotations to masks...\n",
            "Processing 252 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting YOLO to masks: 100%|██████████| 252/252 [00:09<00:00, 25.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 252 images\n",
            "\n",
            "Step 2: Splitting dataset...\n",
            "\n",
            "Step 3: Creating datasets...\n",
            "Found 176 valid image-mask pairs\n",
            "Found 50 valid image-mask pairs\n",
            "\n",
            "Step 4: Training model...\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 88/88 [03:06<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3944, Val Loss: 0.2831\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 88/88 [03:07<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.2122, Val Loss: 0.1805\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 88/88 [03:05<00:00,  2.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.1712, Val Loss: 0.1707\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 88/88 [03:09<00:00,  2.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.1593, Val Loss: 0.2462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 88/88 [03:03<00:00,  2.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.1514, Val Loss: 0.4193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 88/88 [03:03<00:00,  2.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss: 0.1449, Val Loss: 0.1435\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 88/88 [03:02<00:00,  2.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss: 0.1318, Val Loss: 0.1280\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 88/88 [03:04<00:00,  2.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss: 0.1247, Val Loss: 0.1260\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 88/88 [03:03<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss: 0.1223, Val Loss: 0.1228\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 88/88 [03:03<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss: 0.1208, Val Loss: 0.1214\n",
            "New best model saved!\n",
            "\n",
            "=== TRAINING COMPLETED ===\n",
            "Model saved as 'best_tooth_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# ================================\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# ================================\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, num_samples=4):\n",
        "    \"\"\"Visualize model predictions vs ground truth\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Fix mask shape if needed\n",
        "            if len(masks.shape) == 3:\n",
        "                masks = masks.unsqueeze(1)\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(images)\n",
        "            predictions = torch.sigmoid(outputs)  # Convert to probabilities\n",
        "\n",
        "            # Take first image in batch\n",
        "            image = images[0].cpu().numpy().transpose(1, 2, 0)\n",
        "            mask = masks[0, 0].cpu().numpy()\n",
        "            pred = predictions[0, 0].cpu().numpy()\n",
        "\n",
        "            # Denormalize image for display\n",
        "            image = denormalize_image(image)\n",
        "\n",
        "            # Plot results\n",
        "            axes[i, 0].imshow(image)\n",
        "            axes[i, 0].set_title('Original Image')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            axes[i, 1].imshow(mask, cmap='gray')\n",
        "            axes[i, 1].set_title('Ground Truth')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            axes[i, 2].imshow(pred, cmap='gray')\n",
        "            axes[i, 2].set_title('Prediction (Prob)')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "            # Binary prediction (threshold at 0.5)\n",
        "            binary_pred = (pred > 0.5).astype(np.uint8)\n",
        "            axes[i, 3].imshow(binary_pred, cmap='gray')\n",
        "            axes[i, 3].set_title('Binary Prediction')\n",
        "            axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('segmentation_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def denormalize_image(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"Denormalize image for visualization\"\"\"\n",
        "    image = image.copy()\n",
        "    for i in range(3):\n",
        "        image[:, :, i] = image[:, :, i] * std[i] + mean[i]\n",
        "    return np.clip(image, 0, 1)\n",
        "\n",
        "def overlay_prediction_on_image(image, prediction, alpha=0.5):\n",
        "    \"\"\"Overlay prediction mask on original image\"\"\"\n",
        "    # Convert to numpy if needed\n",
        "    if torch.is_tensor(image):\n",
        "        image = image.cpu().numpy().transpose(1, 2, 0)\n",
        "    if torch.is_tensor(prediction):\n",
        "        prediction = prediction.cpu().numpy()\n",
        "\n",
        "    # Denormalize image\n",
        "    image = denormalize_image(image)\n",
        "\n",
        "    # Create colored overlay (red for predictions)\n",
        "    overlay = np.zeros_like(image)\n",
        "    overlay[:, :, 0] = prediction  # Red channel\n",
        "\n",
        "    # Blend with original image\n",
        "    result = image * (1 - alpha) + overlay * alpha\n",
        "    return np.clip(result, 0, 1)\n",
        "\n",
        "# ================================\n",
        "# EVALUATION METRICS\n",
        "# ================================\n",
        "\n",
        "def calculate_iou(pred, target, threshold=0.5):\n",
        "    \"\"\"Calculate Intersection over Union (IoU)\"\"\"\n",
        "    pred_binary = (pred > threshold).float()\n",
        "    target_binary = target.float()\n",
        "\n",
        "    intersection = (pred_binary * target_binary).sum()\n",
        "    union = pred_binary.sum() + target_binary.sum() - intersection\n",
        "\n",
        "    iou = intersection / (union + 1e-8)\n",
        "    return iou.item()\n",
        "\n",
        "def calculate_dice(pred, target, threshold=0.5):\n",
        "    \"\"\"Calculate Dice coefficient\"\"\"\n",
        "    pred_binary = (pred > threshold).float()\n",
        "    target_binary = target.float()\n",
        "\n",
        "    intersection = (pred_binary * target_binary).sum()\n",
        "    dice = (2 * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-8)\n",
        "\n",
        "    return dice.item()\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    total_iou = 0\n",
        "    total_dice = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Fix mask shape if needed\n",
        "            if len(masks.shape) == 3:\n",
        "                masks = masks.unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            predictions = torch.sigmoid(outputs)\n",
        "\n",
        "            # Calculate metrics for each image in batch\n",
        "            for i in range(images.size(0)):\n",
        "                iou = calculate_iou(predictions[i], masks[i])\n",
        "                dice = calculate_dice(predictions[i], masks[i])\n",
        "\n",
        "                total_iou += iou\n",
        "                total_dice += dice\n",
        "                total_samples += 1\n",
        "\n",
        "    avg_iou = total_iou / total_samples\n",
        "    avg_dice = total_dice / total_samples\n",
        "\n",
        "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Average Dice: {avg_dice:.4f}\")\n",
        "\n",
        "    return avg_iou, avg_dice\n",
        "\n",
        "# ================================\n",
        "# PREDICTION ON NEW IMAGES\n",
        "# ================================\n",
        "\n",
        "def predict_on_image(model, image_path, device, transform=None):\n",
        "    \"\"\"Predict segmentation mask for a single image\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        prediction = torch.sigmoid(output)\n",
        "\n",
        "    # Convert to numpy\n",
        "    pred_mask = prediction[0, 0].cpu().numpy()\n",
        "\n",
        "    return pred_mask\n",
        "\n",
        "def save_prediction_results(model, image_path, output_dir, device):\n",
        "    \"\"\"Save prediction results as images\"\"\"\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get prediction\n",
        "    pred_mask = predict_on_image(model, image_path, device)\n",
        "\n",
        "    # Load original image\n",
        "    original_image = Image.open(image_path).convert('RGB')\n",
        "    original_image = original_image.resize((256, 256))\n",
        "\n",
        "    # Save results\n",
        "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "    # Save original\n",
        "    original_image.save(os.path.join(output_dir, f\"{base_name}_original.png\"))\n",
        "\n",
        "    # Save prediction mask\n",
        "    pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "    pred_img.save(os.path.join(output_dir, f\"{base_name}_prediction.png\"))\n",
        "\n",
        "    # Save binary mask\n",
        "    binary_mask = (pred_mask > 0.5).astype(np.uint8) * 255\n",
        "    binary_img = Image.fromarray(binary_mask)\n",
        "    binary_img.save(os.path.join(output_dir, f\"{base_name}_binary.png\"))\n",
        "\n",
        "    # Save overlay\n",
        "    overlay = overlay_prediction_on_image(\n",
        "        np.array(original_image) / 255.0,\n",
        "        pred_mask,\n",
        "        alpha=0.3\n",
        "    )\n",
        "    overlay_img = Image.fromarray((overlay * 255).astype(np.uint8))\n",
        "    overlay_img.save(os.path.join(output_dir, f\"{base_name}_overlay.png\"))\n",
        "\n",
        "    print(f\"Results saved to {output_dir}\")\n",
        "\n",
        "# ================================\n",
        "# USAGE EXAMPLES\n",
        "# ================================\n",
        "\n",
        "def main_evaluation():\n",
        "    \"\"\"Example usage of evaluation functions\"\"\"\n",
        "\n",
        "    # Load your model\n",
        "    model = LightweightUNet(n_channels=3, n_classes=1)\n",
        "    model.load_state_dict(torch.load('best_tooth_model.pth'))\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Assume you have val_loader ready\n",
        "    # val_loader = your_validation_dataloader\n",
        "\n",
        "    # 1. Visualize predictions\n",
        "    print(\"Visualizing predictions...\")\n",
        "    # visualize_predictions(model, val_loader, device, num_samples=4)\n",
        "\n",
        "    # 2. Evaluate model performance\n",
        "    print(\"Evaluating model...\")\n",
        "    # avg_iou, avg_dice = evaluate_model(model, val_loader, device)\n",
        "\n",
        "    # 3. Predict on a single image\n",
        "    print(\"Predicting on single image...\")\n",
        "    # pred_mask = predict_on_image(model, 'path/to/your/image.jpg', device)\n",
        "\n",
        "    # 4. Save results for multiple images\n",
        "    print(\"Saving prediction results...\")\n",
        "    # save_prediction_results(model, 'path/to/image.jpg', 'output_results/', device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzXakZtqNbLA",
        "outputId": "5c34389b-346b-4052-d9ac-83cc33ace060"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing predictions...\n",
            "Evaluating model...\n",
            "Predicting on single image...\n",
            "Saving prediction results...\n"
          ]
        }
      ]
    }
  ]
}